{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "메모리 네트워크 QA",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabie0208/nlp-study/blob/main/%EB%A9%94%EB%AA%A8%EB%A6%AC_%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtP8ZuteNnXu"
      },
      "source": [
        "링크 : https://wikidocs.net/82475"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jJWHK_eANDN0",
        "outputId": "aa946e96-864e-4328-ad7a-eac9e105bfbd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs9zPMoeDZzb"
      },
      "source": [
        "# 메모리 네트워크를 이용한 영어 QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVLCRblwDXEl"
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import tarfile\n",
        "from nltk import FreqDist\n",
        "from functools import reduce\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ2N0w4iFRDX"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmBTrc8BDX9X",
        "outputId": "de87cdb3-f140-4559-b98f-3b1852c01475"
      },
      "source": [
        "path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/'\n",
        "                'babi_tasks_1-20_v1-2.tar.gz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
            "11747328/11745123 [==============================] - 0s 0us/step\n",
            "11755520/11745123 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJj30yVoDYEG"
      },
      "source": [
        "with tarfile.open(path) as tar:\n",
        " tar.extractall()\n",
        " tar.close()\n",
        "\n",
        "DATA_DIR = 'tasks_1-20_v1-2/en-10k'\n",
        "TRAIN_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_train.txt\")\n",
        "TEST_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRxkEc8oFTjb"
      },
      "source": [
        "## Babi 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GciuOfrjDYMH",
        "outputId": "8eda1e0e-2515-45f9-b3c2-99aa9da82509"
      },
      "source": [
        "i = 0\n",
        "lines = open(TRAIN_FILE , \"rb\")\n",
        "for line in lines:\n",
        "    line = line.decode(\"utf-8\").strip()\n",
        "    # lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
        "    i = i + 1\n",
        "    print(line)\n",
        "    if i == 20:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Mary moved to the bathroom.\n",
            "2 John went to the hallway.\n",
            "3 Where is Mary? \tbathroom\t1\n",
            "4 Daniel went back to the hallway.\n",
            "5 Sandra moved to the garden.\n",
            "6 Where is Daniel? \thallway\t4\n",
            "7 John moved to the office.\n",
            "8 Sandra journeyed to the bathroom.\n",
            "9 Where is Daniel? \thallway\t4\n",
            "10 Mary moved to the hallway.\n",
            "11 Daniel travelled to the office.\n",
            "12 Where is Daniel? \toffice\t11\n",
            "13 John went back to the garden.\n",
            "14 John moved to the bedroom.\n",
            "15 Where is Sandra? \tbathroom\t8\n",
            "1 Sandra travelled to the office.\n",
            "2 Sandra went to the bathroom.\n",
            "3 Where is Sandra? \tbathroom\t2\n",
            "4 Mary went to the bedroom.\n",
            "5 Daniel moved to the hallway.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liYRP4mKFWVQ"
      },
      "source": [
        "## 스토리, 질문, 답변 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xZ4hjTPDYPM"
      },
      "source": [
        "def read_data(dir):\n",
        "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
        "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
        "    lines = open(dir, \"rb\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.decode(\"utf-8\") # b' 제거\n",
        "        line = line.strip() # '\\n' 제거\n",
        "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
        "        # 여기까지는 모든 줄에 적용되는 전처리\n",
        "\n",
        "        if int(idx) == 1:\n",
        "            story_temp = []\n",
        "\n",
        "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
        "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
        "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "        else: # 현재 읽는 줄이 스토리인 경우\n",
        "            story_temp.append(text) # 임시 저장\n",
        "\n",
        "    lines.close()\n",
        "    return stories, questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNAOOBbfDYJm"
      },
      "source": [
        "train_data = read_data(TRAIN_FILE)\n",
        "test_data = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNh7jAryDYHI"
      },
      "source": [
        "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
        "test_stories, test_questions, test_answers = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAe5T7j5Dj0R",
        "outputId": "294b4702-8e4c-430d-8c71-bcd91bb95580"
      },
      "source": [
        "print('훈련용 스토리의 개수 :', len(train_stories))\n",
        "print('훈련용 질문의 개수 :',len(train_questions))\n",
        "print('훈련용 답변의 개수 :',len(train_answers))\n",
        "print('테스트용 스토리의 개수 :',len(test_stories))\n",
        "print('테스트용 질문의 개수 :',len(test_questions))\n",
        "print('테스트용 답변의 개수 :',len(test_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련용 스토리의 개수 : 10000\n",
            "훈련용 질문의 개수 : 10000\n",
            "훈련용 답변의 개수 : 10000\n",
            "테스트용 스토리의 개수 : 1000\n",
            "테스트용 질문의 개수 : 1000\n",
            "테스트용 답변의 개수 : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH-whk9FDlF9",
        "outputId": "978bdf69-7af3-40c4-9d77-f9bf308a0d4b"
      },
      "source": [
        "train_stories[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John went back to the garden.',\n",
              " 'Mary went to the kitchen.',\n",
              " 'Sandra went back to the bedroom.',\n",
              " 'John travelled to the bedroom.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fI9AfnQvDl07",
        "outputId": "e41e0f67-6fc4-4091-944a-573e7abcfe36"
      },
      "source": [
        "train_questions[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Where is John? '"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c-gE6Y2CDm1W",
        "outputId": "6f2a8e07-1392-4c64-e556-cd77ec5dae1a"
      },
      "source": [
        "train_answers[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bedroom'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6z6RxyFez9"
      },
      "source": [
        "## 단어 집합 생성 및 토큰화 및 스토리와 질문의 최대 길이 구하기  \n",
        "단, 스토리는 모두 펼쳐서 하나의 샘플 처리."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE6GiR7DDokB"
      },
      "source": [
        "def tokenize(sent):\n",
        "    return [ x.strip() for x in re.split('(\\W+)', sent) if x and x.strip()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HykreiVoDp_Q"
      },
      "source": [
        "def preprocess_data(train_data, test_data):\n",
        "    counter = FreqDist()\n",
        "\n",
        "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    # 각 샘플의 길이를 저장하는 리스트\n",
        "    story_len = []\n",
        "    question_len = []\n",
        "\n",
        "    for stories, questions, answers in [train_data, test_data]:\n",
        "        for story in stories:\n",
        "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
        "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
        "            for word in stories: # 단어 집합에 단어 추가\n",
        "                counter[word] += 1\n",
        "        for question in questions:\n",
        "            question = tokenize(question)\n",
        "            question_len.append(len(question))\n",
        "            for word in question:\n",
        "                counter[word] += 1\n",
        "        for answer in answers:\n",
        "            answer = tokenize(answer)\n",
        "            for word in answer:\n",
        "                counter[word] += 1\n",
        "\n",
        "    # 단어 집합 생성\n",
        "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
        "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
        "\n",
        "    # 가장 긴 샘플의 길이\n",
        "    story_max_len = np.max(story_len)\n",
        "    question_max_len = np.max(question_len)\n",
        "\n",
        "    return word2idx, idx2word, story_max_len, question_max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFzvZx8PDrik"
      },
      "source": [
        "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss6DWK9IDtIr",
        "outputId": "075b4bbc-30a7-4045-ec3f-d3fd69974225"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'to': 1, 'the': 2, '.': 3, 'went': 4, 'Sandra': 5, 'John': 6, 'Daniel': 7, 'Mary': 8, 'travelled': 9, 'journeyed': 10, 'back': 11, 'bathroom': 12, 'garden': 13, 'hallway': 14, 'moved': 15, 'office': 16, 'kitchen': 17, 'bedroom': 18, 'Where': 19, 'is': 20, '?': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbGiMRvuDu1k"
      },
      "source": [
        "vocab_size = len(word2idx) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7x14T1-DwC8",
        "outputId": "ba8315aa-bc61-4780-fd66-f8c99168e325"
      },
      "source": [
        "print('스토리의 최대 길이 :',story_max_len)\n",
        "print('질문의 최대 길이 :',question_max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스토리의 최대 길이 : 68\n",
            "질문의 최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXGyRWzuF4Df"
      },
      "source": [
        "## 정수 인코딩 및 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zqvr2MADxOg"
      },
      "source": [
        "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
        "    Xs, Xq, Y = [], [], []\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    stories, questions, answers = data\n",
        "    for story, question, answer in zip(stories, questions, answers):\n",
        "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
        "        xq = [word2idx[w] for w in tokenize(question)]\n",
        "        Xs.append(xs)\n",
        "        Xq.append(xq)\n",
        "        Y.append(word2idx[answer])\n",
        "\n",
        "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
        "        # 정답은 원-핫 인코딩\n",
        "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
        "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
        "           to_categorical(Y, num_classes=len(word2idx) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbKaunR_Dyjh"
      },
      "source": [
        "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
        "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUiYGluHD0s2",
        "outputId": "a4a6e5b7-358f-41bc-b420-367972089d4a"
      },
      "source": [
        "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 68) (10000, 4) (10000, 22) (1000, 68) (1000, 4) (1000, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aucZyNPSD2eM",
        "outputId": "52f3a35a-e4a9-4a2a-ca8f-496ea4400693"
      },
      "source": [
        "Xstrain[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  6,  4, 11,  1,  2, 13,  3,  8,  4,\n",
              "        1,  2, 17,  3,  5,  4, 11,  1,  2, 18,  3,  6,  9,  1,  2, 18,  3],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XusaXO8LD6d8",
        "outputId": "e53a9dd3-faab-43d7-d54b-8ef5ced86a06"
      },
      "source": [
        "Xqtrain[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19, 20,  6, 21], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVRkfkJwD-Kj",
        "outputId": "d8a305d0-5346-49c4-e455-54fba512e3a2"
      },
      "source": [
        "Ytrain[3576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdj_11K2F6Rt"
      },
      "source": [
        "## 메모리 네트워크 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6XWVvzPEApv"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVBqicL_EB2f"
      },
      "source": [
        "# 에포크 횟수\n",
        "train_epochs = 120\n",
        "# 배치 크기\n",
        "batch_size = 32\n",
        "# 임베딩 크기\n",
        "embed_size = 50\n",
        "# LSTM의 크기\n",
        "lstm_size = 64\n",
        "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
        "dropout_rate = 0.30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8saI92oEC86",
        "outputId": "3601d73c-c84b-427e-853a-3cecc458452a"
      },
      "source": [
        "# 플레이스 홀더. 입력을 담는 변수\n",
        "input_sequence = Input((story_max_len,))\n",
        "question = Input((question_max_len,))\n",
        "\n",
        "print('Stories :', input_sequence)\n",
        "print('Question:', question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stories : KerasTensor(type_spec=TensorSpec(shape=(None, 68), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\")\n",
            "Question: KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2gNvy-wEECC"
      },
      "source": [
        "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embed_size))\n",
        "input_encoder_m.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
        "\n",
        "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
        "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=question_max_len))\n",
        "input_encoder_c.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnNOXEPDEGJj"
      },
      "source": [
        "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=embed_size,\n",
        "                               input_length=question_max_len))\n",
        "question_encoder.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukaymDiSEHcs",
        "outputId": "7614db8f-897a-4722-e1f3-e72e36a72821"
      },
      "source": [
        "# 실질적인 임베딩 과정\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)\n",
        "\n",
        "print('Input encoded m', input_encoded_m)\n",
        "print('Input encoded c', input_encoded_c)\n",
        "print('Question encoded', question_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input encoded m KerasTensor(type_spec=TensorSpec(shape=(None, 68, 50), dtype=tf.float32, name=None), name='sequential_3/dropout_4/Identity:0', description=\"created by layer 'sequential_3'\")\n",
            "Input encoded c KerasTensor(type_spec=TensorSpec(shape=(None, 68, 4), dtype=tf.float32, name=None), name='sequential_4/dropout_5/Identity:0', description=\"created by layer 'sequential_4'\")\n",
            "Question encoded KerasTensor(type_spec=TensorSpec(shape=(None, 4, 50), dtype=tf.float32, name=None), name='sequential_5/dropout_6/Identity:0', description=\"created by layer 'sequential_5'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Gcx9SEItb",
        "outputId": "fff54d50-3854-4dae-ba8c-12a9319c79d2"
      },
      "source": [
        "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
        "# 유사도는 내적을 사용한다.\n",
        "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
        "match = Activation('softmax')(match)\n",
        "print('Match shape', match)\n",
        "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match shape KerasTensor(type_spec=TensorSpec(shape=(None, 68, 4), dtype=tf.float32, name=None), name='activation_2/Softmax:0', description=\"created by layer 'activation_2'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSdoMHmrEMBR"
      },
      "source": [
        "# 유사도가 반영된 어텐션 분포 행렬과 임베딩 C를 거친 스토리 행렬을 더한다.\n",
        "# 두 행렬 모두 크기는 (68, 4)이다.\n",
        "# 이로부터 얻은 행렬은 어텐션 값 행렬(Attention Value Matrix)이다.\n",
        "response = add([match, input_encoded_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtvioXUaENXL",
        "outputId": "d91c6c08-6f82-42c1-ba1d-158d4c122ec9"
      },
      "source": [
        "# 질문 행렬은 (4, 50)의 크기를 가진다.\n",
        "# 하지만 어텐션 값 행렬의 크기는 (68, 4)이다.\n",
        "# 이 두 개를 연결시켜주기 위해서 어텐션 값 행렬의 크기를 (4, 68)로 변환해준다.\n",
        "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
        "print('Response shape', response)\n",
        "\n",
        "# 질문 행렬과 어텐션 값 행렬을 연결한다.\n",
        "# (4, 118)의 크기를 가진다.\n",
        "answer = concatenate([response, question_encoded])\n",
        "print('Answer shape', answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response shape KerasTensor(type_spec=TensorSpec(shape=(None, 4, 68), dtype=tf.float32, name=None), name='permute_1/transpose:0', description=\"created by layer 'permute_1'\")\n",
            "Answer shape KerasTensor(type_spec=TensorSpec(shape=(None, 4, 118), dtype=tf.float32, name=None), name='concatenate_1/concat:0', description=\"created by layer 'concatenate_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ-WVFYOE3Hl"
      },
      "source": [
        "answer = LSTM(lstm_size)(answer)\n",
        "answer = Dropout(dropout_rate)(answer)\n",
        "answer = Dense(vocab_size)(answer)\n",
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbRKnZa5E6A7",
        "outputId": "1703547d-8d32-426e-e8b7-f7657511c79f"
      },
      "source": [
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# start training the model\n",
        "history = model.fit([Xstrain, Xqtrain],\n",
        "         Ytrain, batch_size, train_epochs,\n",
        "         validation_data=([Xstest, Xqtest], Ytest))\n",
        "\n",
        "# save model\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 68)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, None, 50)     1100        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 4, 50)        1100        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 68, 4)        0           sequential_3[0][0]               \n",
            "                                                                 sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 68, 4)        0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, None, 4)      88          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 68, 4)        0           activation_2[0][0]               \n",
            "                                                                 sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 4, 68)        0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 118)       0           permute_1[0][0]                  \n",
            "                                                                 sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 64)           46848       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 22)           1430        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 22)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 50,566\n",
            "Trainable params: 50,566\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/120\n",
            "313/313 [==============================] - 5s 11ms/step - loss: 1.8877 - acc: 0.1724 - val_loss: 1.8049 - val_acc: 0.2210\n",
            "Epoch 2/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.7108 - acc: 0.2643 - val_loss: 1.6009 - val_acc: 0.3600\n",
            "Epoch 3/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5561 - acc: 0.3709 - val_loss: 1.4884 - val_acc: 0.4200\n",
            "Epoch 4/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.5015 - acc: 0.4086 - val_loss: 1.4990 - val_acc: 0.3980\n",
            "Epoch 5/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4530 - acc: 0.4384 - val_loss: 1.3904 - val_acc: 0.4500\n",
            "Epoch 6/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3999 - acc: 0.4591 - val_loss: 1.3510 - val_acc: 0.4940\n",
            "Epoch 7/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3768 - acc: 0.4709 - val_loss: 1.3212 - val_acc: 0.5000\n",
            "Epoch 8/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3583 - acc: 0.4708 - val_loss: 1.3016 - val_acc: 0.5010\n",
            "Epoch 9/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3286 - acc: 0.4838 - val_loss: 1.2831 - val_acc: 0.5010\n",
            "Epoch 10/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3107 - acc: 0.4944 - val_loss: 1.2750 - val_acc: 0.5190\n",
            "Epoch 11/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2940 - acc: 0.4998 - val_loss: 1.2568 - val_acc: 0.5230\n",
            "Epoch 12/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.2707 - acc: 0.5049 - val_loss: 1.2204 - val_acc: 0.5290\n",
            "Epoch 13/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.2595 - acc: 0.5093 - val_loss: 1.2194 - val_acc: 0.5270\n",
            "Epoch 14/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.2229 - acc: 0.5138 - val_loss: 1.2206 - val_acc: 0.5070\n",
            "Epoch 15/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.2118 - acc: 0.5155 - val_loss: 1.1828 - val_acc: 0.5170\n",
            "Epoch 16/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1873 - acc: 0.5152 - val_loss: 1.1586 - val_acc: 0.5250\n",
            "Epoch 17/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1781 - acc: 0.5151 - val_loss: 1.1730 - val_acc: 0.5050\n",
            "Epoch 18/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1672 - acc: 0.5186 - val_loss: 1.1529 - val_acc: 0.5170\n",
            "Epoch 19/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1517 - acc: 0.5211 - val_loss: 1.1529 - val_acc: 0.5150\n",
            "Epoch 20/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1461 - acc: 0.5247 - val_loss: 1.1443 - val_acc: 0.5150\n",
            "Epoch 21/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1428 - acc: 0.5265 - val_loss: 1.1519 - val_acc: 0.5200\n",
            "Epoch 22/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1316 - acc: 0.5323 - val_loss: 1.1497 - val_acc: 0.5210\n",
            "Epoch 23/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1291 - acc: 0.5267 - val_loss: 1.1438 - val_acc: 0.5190\n",
            "Epoch 24/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1164 - acc: 0.5321 - val_loss: 1.1535 - val_acc: 0.5060\n",
            "Epoch 25/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1101 - acc: 0.5319 - val_loss: 1.1574 - val_acc: 0.5190\n",
            "Epoch 26/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1042 - acc: 0.5400 - val_loss: 1.1579 - val_acc: 0.5170\n",
            "Epoch 27/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0969 - acc: 0.5353 - val_loss: 1.1397 - val_acc: 0.5190\n",
            "Epoch 28/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0861 - acc: 0.5417 - val_loss: 1.1555 - val_acc: 0.5040\n",
            "Epoch 29/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0839 - acc: 0.5424 - val_loss: 1.1599 - val_acc: 0.5180\n",
            "Epoch 30/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0757 - acc: 0.5442 - val_loss: 1.1604 - val_acc: 0.4990\n",
            "Epoch 31/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0690 - acc: 0.5504 - val_loss: 1.1512 - val_acc: 0.5200\n",
            "Epoch 32/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0629 - acc: 0.5496 - val_loss: 1.1493 - val_acc: 0.5070\n",
            "Epoch 33/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0475 - acc: 0.5583 - val_loss: 1.1461 - val_acc: 0.5220\n",
            "Epoch 34/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0388 - acc: 0.5643 - val_loss: 1.1278 - val_acc: 0.5230\n",
            "Epoch 35/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0107 - acc: 0.5835 - val_loss: 1.1107 - val_acc: 0.5590\n",
            "Epoch 36/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9670 - acc: 0.6172 - val_loss: 1.0744 - val_acc: 0.5950\n",
            "Epoch 37/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.8831 - acc: 0.6646 - val_loss: 0.9027 - val_acc: 0.6610\n",
            "Epoch 38/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.7357 - acc: 0.7367 - val_loss: 0.6828 - val_acc: 0.7500\n",
            "Epoch 39/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.6191 - acc: 0.7779 - val_loss: 0.6236 - val_acc: 0.7610\n",
            "Epoch 40/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.5601 - acc: 0.7939 - val_loss: 0.5961 - val_acc: 0.7780\n",
            "Epoch 41/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4973 - acc: 0.8215 - val_loss: 0.5355 - val_acc: 0.7980\n",
            "Epoch 42/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4361 - acc: 0.8343 - val_loss: 0.4578 - val_acc: 0.8030\n",
            "Epoch 43/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3944 - acc: 0.8519 - val_loss: 0.4131 - val_acc: 0.8320\n",
            "Epoch 44/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3675 - acc: 0.8653 - val_loss: 0.3976 - val_acc: 0.8420\n",
            "Epoch 45/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3374 - acc: 0.8720 - val_loss: 0.4072 - val_acc: 0.8340\n",
            "Epoch 46/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3300 - acc: 0.8768 - val_loss: 0.3840 - val_acc: 0.8400\n",
            "Epoch 47/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3235 - acc: 0.8790 - val_loss: 0.3760 - val_acc: 0.8440\n",
            "Epoch 48/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3059 - acc: 0.8838 - val_loss: 0.3600 - val_acc: 0.8570\n",
            "Epoch 49/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2889 - acc: 0.8906 - val_loss: 0.3868 - val_acc: 0.8430\n",
            "Epoch 50/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2878 - acc: 0.8909 - val_loss: 0.3419 - val_acc: 0.8590\n",
            "Epoch 51/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2739 - acc: 0.8941 - val_loss: 0.3559 - val_acc: 0.8560\n",
            "Epoch 52/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2650 - acc: 0.9028 - val_loss: 0.3339 - val_acc: 0.8660\n",
            "Epoch 53/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2571 - acc: 0.9074 - val_loss: 0.3164 - val_acc: 0.8760\n",
            "Epoch 54/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2403 - acc: 0.9089 - val_loss: 0.3094 - val_acc: 0.8820\n",
            "Epoch 55/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2306 - acc: 0.9127 - val_loss: 0.3409 - val_acc: 0.8720\n",
            "Epoch 56/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2066 - acc: 0.9197 - val_loss: 0.2919 - val_acc: 0.8960\n",
            "Epoch 57/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2053 - acc: 0.9245 - val_loss: 0.2717 - val_acc: 0.8930\n",
            "Epoch 58/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1891 - acc: 0.9317 - val_loss: 0.2551 - val_acc: 0.9140\n",
            "Epoch 59/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1726 - acc: 0.9366 - val_loss: 0.2386 - val_acc: 0.9230\n",
            "Epoch 60/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1694 - acc: 0.9417 - val_loss: 0.2530 - val_acc: 0.9120\n",
            "Epoch 61/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1561 - acc: 0.9434 - val_loss: 0.2336 - val_acc: 0.9160\n",
            "Epoch 62/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1477 - acc: 0.9470 - val_loss: 0.2135 - val_acc: 0.9270\n",
            "Epoch 63/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1426 - acc: 0.9520 - val_loss: 0.2272 - val_acc: 0.9220\n",
            "Epoch 64/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1309 - acc: 0.9520 - val_loss: 0.2229 - val_acc: 0.9260\n",
            "Epoch 65/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1313 - acc: 0.9553 - val_loss: 0.1872 - val_acc: 0.9370\n",
            "Epoch 66/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1208 - acc: 0.9565 - val_loss: 0.2153 - val_acc: 0.9290\n",
            "Epoch 67/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1206 - acc: 0.9584 - val_loss: 0.1979 - val_acc: 0.9290\n",
            "Epoch 68/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1208 - acc: 0.9578 - val_loss: 0.1915 - val_acc: 0.9300\n",
            "Epoch 69/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1129 - acc: 0.9611 - val_loss: 0.1853 - val_acc: 0.9380\n",
            "Epoch 70/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0995 - acc: 0.9648 - val_loss: 0.1852 - val_acc: 0.9320\n",
            "Epoch 71/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0993 - acc: 0.9632 - val_loss: 0.1984 - val_acc: 0.9300\n",
            "Epoch 72/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0943 - acc: 0.9695 - val_loss: 0.2176 - val_acc: 0.9320\n",
            "Epoch 73/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0938 - acc: 0.9669 - val_loss: 0.1850 - val_acc: 0.9410\n",
            "Epoch 74/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0838 - acc: 0.9706 - val_loss: 0.2001 - val_acc: 0.9370\n",
            "Epoch 75/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0821 - acc: 0.9723 - val_loss: 0.1827 - val_acc: 0.9390\n",
            "Epoch 76/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0814 - acc: 0.9717 - val_loss: 0.2061 - val_acc: 0.9410\n",
            "Epoch 77/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0746 - acc: 0.9746 - val_loss: 0.2092 - val_acc: 0.9370\n",
            "Epoch 78/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0790 - acc: 0.9741 - val_loss: 0.2033 - val_acc: 0.9360\n",
            "Epoch 79/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0706 - acc: 0.9763 - val_loss: 0.1823 - val_acc: 0.9390\n",
            "Epoch 80/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0708 - acc: 0.9781 - val_loss: 0.1917 - val_acc: 0.9350\n",
            "Epoch 81/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0600 - acc: 0.9797 - val_loss: 0.2076 - val_acc: 0.9400\n",
            "Epoch 82/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0603 - acc: 0.9800 - val_loss: 0.2074 - val_acc: 0.9440\n",
            "Epoch 83/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0633 - acc: 0.9788 - val_loss: 0.1742 - val_acc: 0.9470\n",
            "Epoch 84/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0565 - acc: 0.9816 - val_loss: 0.1880 - val_acc: 0.9470\n",
            "Epoch 85/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0604 - acc: 0.9806 - val_loss: 0.1941 - val_acc: 0.9510\n",
            "Epoch 86/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0570 - acc: 0.9819 - val_loss: 0.1759 - val_acc: 0.9470\n",
            "Epoch 87/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0518 - acc: 0.9829 - val_loss: 0.1723 - val_acc: 0.9550\n",
            "Epoch 88/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0576 - acc: 0.9829 - val_loss: 0.1804 - val_acc: 0.9480\n",
            "Epoch 89/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0549 - acc: 0.9829 - val_loss: 0.1866 - val_acc: 0.9490\n",
            "Epoch 90/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0473 - acc: 0.9835 - val_loss: 0.1723 - val_acc: 0.9510\n",
            "Epoch 91/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0519 - acc: 0.9836 - val_loss: 0.1796 - val_acc: 0.9540\n",
            "Epoch 92/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0498 - acc: 0.9849 - val_loss: 0.1689 - val_acc: 0.9500\n",
            "Epoch 93/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0500 - acc: 0.9864 - val_loss: 0.1810 - val_acc: 0.9460\n",
            "Epoch 94/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0457 - acc: 0.9873 - val_loss: 0.1840 - val_acc: 0.9470\n",
            "Epoch 95/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0409 - acc: 0.9877 - val_loss: 0.1794 - val_acc: 0.9550\n",
            "Epoch 96/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0436 - acc: 0.9870 - val_loss: 0.1614 - val_acc: 0.9520\n",
            "Epoch 97/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0409 - acc: 0.9877 - val_loss: 0.1615 - val_acc: 0.9560\n",
            "Epoch 98/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0420 - acc: 0.9871 - val_loss: 0.1724 - val_acc: 0.9530\n",
            "Epoch 99/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0419 - acc: 0.9882 - val_loss: 0.1845 - val_acc: 0.9560\n",
            "Epoch 100/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0424 - acc: 0.9882 - val_loss: 0.1817 - val_acc: 0.9490\n",
            "Epoch 101/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0400 - acc: 0.9879 - val_loss: 0.1858 - val_acc: 0.9520\n",
            "Epoch 102/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0396 - acc: 0.9872 - val_loss: 0.1907 - val_acc: 0.9520\n",
            "Epoch 103/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0359 - acc: 0.9896 - val_loss: 0.1750 - val_acc: 0.9530\n",
            "Epoch 104/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0424 - acc: 0.9871 - val_loss: 0.2003 - val_acc: 0.9540\n",
            "Epoch 105/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0400 - acc: 0.9875 - val_loss: 0.1687 - val_acc: 0.9590\n",
            "Epoch 106/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0408 - acc: 0.9873 - val_loss: 0.2029 - val_acc: 0.9500\n",
            "Epoch 107/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0376 - acc: 0.9891 - val_loss: 0.1709 - val_acc: 0.9560\n",
            "Epoch 108/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0328 - acc: 0.9895 - val_loss: 0.2114 - val_acc: 0.9550\n",
            "Epoch 109/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0358 - acc: 0.9899 - val_loss: 0.1699 - val_acc: 0.9520\n",
            "Epoch 110/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0384 - acc: 0.9885 - val_loss: 0.2010 - val_acc: 0.9550\n",
            "Epoch 111/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0372 - acc: 0.9888 - val_loss: 0.1588 - val_acc: 0.9620\n",
            "Epoch 112/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0327 - acc: 0.9906 - val_loss: 0.2254 - val_acc: 0.9490\n",
            "Epoch 113/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0385 - acc: 0.9904 - val_loss: 0.1756 - val_acc: 0.9540\n",
            "Epoch 114/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.1733 - val_acc: 0.9580\n",
            "Epoch 115/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0330 - acc: 0.9908 - val_loss: 0.2231 - val_acc: 0.9510\n",
            "Epoch 116/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0286 - acc: 0.9920 - val_loss: 0.1786 - val_acc: 0.9580\n",
            "Epoch 117/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0376 - acc: 0.9913 - val_loss: 0.1623 - val_acc: 0.9660\n",
            "Epoch 118/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0286 - acc: 0.9914 - val_loss: 0.2017 - val_acc: 0.9550\n",
            "Epoch 119/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0288 - acc: 0.9899 - val_loss: 0.1778 - val_acc: 0.9610\n",
            "Epoch 120/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0365 - acc: 0.9907 - val_loss: 0.1666 - val_acc: 0.9580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u8ZjzcRE9L1",
        "outputId": "31da59d0-5d2e-4d97-802b-12199478c4df"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate([Xstest, Xqtest], Ytest)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1666 - acc: 0.9580\n",
            "\n",
            " 테스트 정확도: 0.9580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "1FAs5IVLE-t3",
        "outputId": "e6a00dcc-eb98-41c7-cfb9-66fe17acbf1d"
      },
      "source": [
        "# plot accuracy and loss plot\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# labels\n",
        "ytest = np.argmax(Ytest, axis=1)\n",
        "\n",
        "# get predictions\n",
        "Ytest_ = model.predict([Xstest, Xqtest])\n",
        "ytest_ = np.argmax(Ytest_, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktt34R7EGRZM"
      },
      "source": [
        "# 메모리 네트워크를 이용한 한국어 QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq0aHOs0GWCP"
      },
      "source": [
        "데이터는 각각 아래의 링크에서 다운로드 할 수 있습니다.  \n",
        "훈련 데이터 : https://bit.ly/31SqtHy  \n",
        "테스트 데이터 : https://bit.ly/3f7rH5g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9XiEZRvGXJJ"
      },
      "source": [
        "## 단어 사전 등록이 간편한 형태소 분석기 customized_konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Us3UZp-tQPmT",
        "outputId": "cd573911-4ddc-4ff2-f901-14e41a3ca595"
      },
      "source": [
        "pip install customized_konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: customized_konlpy in /usr/local/lib/python3.6/dist-packages (0.0.64)\n",
            "Requirement already satisfied: Jpype1>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from customized_konlpy) (0.7.5)\n",
            "Requirement already satisfied: konlpy>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from customized_konlpy) (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (1.18.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (0.4.3)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.6.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2020.6.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEgY4ShzQudC"
      },
      "source": [
        "from ckonlpy.tag import Twitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HHNd8PuTQoQK",
        "outputId": "77716e12-9d05-4204-95e0-e98a4abf2d2f"
      },
      "source": [
        "twitter = Twitter()\n",
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
            ]
          },
          "execution_count": 101,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "3Oke4Hd8Qr0r",
        "outputId": "4dd39abb-4cf5-45c5-b98f-7ffb3a4b7cb7"
      },
      "source": [
        "twitter.add_dictionary('은경이', 'Noun')\n",
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['은경이', '는', '사무실', '로', '갔습니다', '.']"
            ]
          },
          "execution_count": 102,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDopEhHUGb7C"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bkg6sr9QQMy"
      },
      "source": [
        "from ckonlpy.tag import Twitter\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from nltk import FreqDist\n",
        "from functools import reduce\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4A77lkgQFzJ"
      },
      "source": [
        "TRAIN_FILE = os.path.join(\"qa1_single-supporting-fact_train_kor.txt\")\n",
        "TEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Cm6WlUGdkr"
      },
      "source": [
        "## Babi 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "H0RfBIooRJqq",
        "outputId": "282ca1c3-0316-45a3-c1fd-bf562861f899"
      },
      "source": [
        "i = 0\n",
        "lines = open(TRAIN_FILE , \"rb\")\n",
        "for line in lines:\n",
        "    line = line.decode(\"utf-8\").strip()\n",
        "    # lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
        "    i = i + 1\n",
        "    print(line)\n",
        "    if i == 20:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 필웅이는 화장실로 갔습니다.\n",
            "2 은경이는 복도로 이동했습니다.\n",
            "3 필웅이는 어디야? \t화장실\t1\n",
            "4 수종이는 복도로 복귀했습니다.\n",
            "5 경임이는 정원으로 갔습니다.\n",
            "6 수종이는 어디야? \t복도\t4\n",
            "7 은경이는 사무실로 갔습니다.\n",
            "8 경임이는 화장실로 뛰어갔습니다.\n",
            "9 수종이는 어디야? \t복도\t4\n",
            "10 필웅이는 복도로 갔습니다.\n",
            "11 수종이는 사무실로 가버렸습니다.\n",
            "12 수종이는 어디야? \t사무실\t11\n",
            "13 은경이는 정원으로 복귀했습니다.\n",
            "14 은경이는 침실로 갔습니다.\n",
            "15 경임이는 어디야? \t화장실\t8\n",
            "1 경임이는 사무실로 가버렸습니다.\n",
            "2 경임이는 화장실로 이동했습니다.\n",
            "3 경임이는 어디야? \t화장실\t2\n",
            "4 필웅이는 침실로 이동했습니다.\n",
            "5 수종이는 복도로 갔습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMiWML_tGfLY"
      },
      "source": [
        "## 스토리, 질문, 답변 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_IwuvrBRNeA"
      },
      "source": [
        "def read_data(dir):\n",
        "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
        "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
        "    lines = open(dir, \"rb\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.decode(\"utf-8\") # b' 제거\n",
        "        line = line.strip() # '\\n' 제거\n",
        "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
        "        # 여기까지는 모든 줄에 적용되는 전처리\n",
        "\n",
        "        if int(idx) == 1:\n",
        "            story_temp = []\n",
        "\n",
        "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
        "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
        "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "        else: # 현재 읽는 줄이 스토리인 경우\n",
        "            story_temp.append(text) # 임시 저장\n",
        "\n",
        "    lines.close()\n",
        "    return stories, questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RabIlPFPWgaa"
      },
      "source": [
        "train_data = read_data(TRAIN_FILE)\n",
        "test_data = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGPZuNgMRQHg"
      },
      "source": [
        "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
        "test_stories, test_questions, test_answers = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "enIOcAh2RRbw",
        "outputId": "343516d3-ae0e-4316-cecd-7774a4a030e3"
      },
      "source": [
        "print('훈련용 스토리의 개수 :', len(train_stories))\n",
        "print('훈련용 질문의 개수 :',len(train_questions))\n",
        "print('훈련용 답변의 개수 :',len(train_answers))\n",
        "print('테스트용 스토리의 개수 :',len(test_stories))\n",
        "print('테스트용 질문의 개수 :',len(test_questions))\n",
        "print('테스트용 답변의 개수 :',len(test_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련용 스토리의 개수 : 10000\n",
            "훈련용 질문의 개수 : 10000\n",
            "훈련용 답변의 개수 : 10000\n",
            "테스트용 스토리의 개수 : 1000\n",
            "테스트용 질문의 개수 : 1000\n",
            "테스트용 답변의 개수 : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "D8Wm1KHjRSjI",
        "outputId": "a955ddfa-671a-4480-ce1b-d84a1ffc9c3f"
      },
      "source": [
        "train_stories[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['은경이는 부엌으로 가버렸습니다.',\n",
              " '필웅이는 사무실로 가버렸습니다.',\n",
              " '수종이는 복도로 뛰어갔습니다.',\n",
              " '은경이는 사무실로 복귀했습니다.',\n",
              " '경임이는 사무실로 이동했습니다.',\n",
              " '경임이는 침실로 갔습니다.']"
            ]
          },
          "execution_count": 111,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Thg7HE61RT14",
        "outputId": "2f8fdaf3-5d82-4a26-e799-07b1564b445c"
      },
      "source": [
        "train_questions[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'은경이는 어디야? '"
            ]
          },
          "execution_count": 112,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "a_bB2m_HRXIA",
        "outputId": "e6fab148-0e26-4a3a-c4df-5ce7ed7a942e"
      },
      "source": [
        "train_answers[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'사무실'"
            ]
          },
          "execution_count": 113,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnDC9aq2GiPZ"
      },
      "source": [
        "## 단어 집합 생성 및 토큰화 및 스토리와 질문의 최대 길이 구하기  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD2OBre7GzQZ"
      },
      "source": [
        "이제 토큰화 함수를 정의하고, 이로부터 Vocabulary를 생성하는 함수를 만들어봅시다. 아래의 함수는 영어 데이터셋에 사용했던 토큰화 함수와 동일합니다. 현재는 한국어이므로 아래의 토큰화 함수를 그대로 사용하는 것은 바람직하지는 않지만, 임시로 사용해보겠습니다. 어절 단위로 했을 때 어떤 단어들이 있는지 출력해보기 위함입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJGkCdQ4WBRi"
      },
      "source": [
        "def tokenize(sent):\n",
        "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiMTed3WWCD7"
      },
      "source": [
        "def preprocess_data(train_data, test_data):\n",
        "    counter = FreqDist()\n",
        "\n",
        "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    # 각 샘플의 길이를 저장하는 리스트\n",
        "    story_len = []\n",
        "    question_len = []\n",
        "\n",
        "    for stories, questions, answers in [train_data, test_data]:\n",
        "        for story in stories:\n",
        "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
        "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
        "            for word in stories: # 단어 집합에 단어 추가\n",
        "                counter[word] += 1\n",
        "        for question in questions:\n",
        "            question = tokenize(question)\n",
        "            question_len.append(len(question))\n",
        "            for word in question:\n",
        "                counter[word] += 1\n",
        "        for answer in answers:\n",
        "            answer = tokenize(answer)\n",
        "            for word in answer:\n",
        "                counter[word] += 1\n",
        "\n",
        "    # 단어 집합 생성\n",
        "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
        "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
        "\n",
        "    # 가장 긴 샘플의 길이\n",
        "    story_max_len = np.max(story_len)\n",
        "    question_max_len = np.max(question_len)\n",
        "\n",
        "    return word2idx, idx2word, story_max_len, question_max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dlGJpYGuqH"
      },
      "source": [
        "word2idx를 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UMQLgV1oWF1k",
        "outputId": "98b1d9c7-59d0-46dc-ebaf-3377141a818d"
      },
      "source": [
        "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3erH8vSjWjsP",
        "outputId": "25329c95-05dc-4c61-8dfc-a849ad884793"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'.': 1, '경임이는': 2, '은경이는': 3, '수종이는': 4, '필웅이는': 5, '이동했습니다': 6, '가버렸습니다': 7, '뛰어갔습니다': 8, '복귀했습니다': 9, '갔습니다': 10, '화장실로': 11, '정원으로': 12, '복도로': 13, '어디야': 14, '?': 15, '부엌으로': 16, '사무실로': 17, '침실로': 18, '화장실': 19, '정원': 20, '사무실': 21, '침실': 22, '복도': 23, '부엌': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-6vL5XG6Im"
      },
      "source": [
        "띄어쓰기 단위, 다시 말해 어절 단위로 했을 때 나오는 총 토큰의 수는 24개입니다. 19번 토큰부터 24번 토큰까지를 봤을 때 장소에 해당되는 명사들은 '화장실', '정원', '사무실', '침실', '복도', '부엌'이 있는 것 같습니다. 그렇다면, 11번 토큰부터 19번 토큰 사이에 등장하는 '화장실로', '정원으로', '복도로', '부엌으로', '사무실로', '침실로'로 분리된 토큰들은 형태소 분석을 하였을 때 전부 '화장실', '정원', '사무실', '침실', '복도', '부엌'으로 분리되어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYXcFUZTGsBT"
      },
      "source": [
        "## 형태소 분석기 사전 등록"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwR3nwaXQwo"
      },
      "source": [
        "'화장실로, 정원으로, 복도로, 부엌으로, 사무실로, 침실로'가 형태소 분석기가 정상적으로 분리하는지 확인이 필요함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "i2RE7DQ9RYMY",
        "outputId": "f17256c2-3e19-4704-f06e-10001e38b491"
      },
      "source": [
        "twitter = Twitter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "A0F7PjuyRtew",
        "outputId": "77f1e77a-a572-419e-b825-80ad58707d9a"
      },
      "source": [
        "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
        "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
        "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
        "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
        "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
        "print(twitter.morphs('은경이는 침실로 갔습니다.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['은', '경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
            "['경', '임', '이', '는', '정원', '으로', '가버렸습니다', '.']\n",
            "['수종', '이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
            "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
            "['수종', '이', '는', '사무실', '로', '갔습니다', '.']\n",
            "['은', '경이', '는', '침실', '로', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsYWIKfRVsKS"
      },
      "source": [
        "twitter.add_dictionary('은경이', 'Noun')\n",
        "twitter.add_dictionary('경임이', 'Noun')\n",
        "twitter.add_dictionary('수종이', 'Noun')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "uYUFcbCBbbA-",
        "outputId": "50f45174-66ec-4700-86ae-aae279b36157"
      },
      "source": [
        "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
        "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
        "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
        "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
        "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
        "print(twitter.morphs('은경이는 침실로 갔습니다.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['은경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
            "['경임이', '는', '정원', '으로', '가버렸습니다', '.']\n",
            "['수종이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
            "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
            "['수종이', '는', '사무실', '로', '갔습니다', '.']\n",
            "['은경이', '는', '침실', '로', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSLbfiHgHCF5"
      },
      "source": [
        "##단어 집합 생성 및 토큰화 및 스토리와 질문의 최대 길이 구하기(다시)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJr8X5lPV0DP"
      },
      "source": [
        "def tokenize(sent):\n",
        "    return twitter.morphs(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY_OqDtUbLMZ"
      },
      "source": [
        "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3j5FIQQGbRFI",
        "outputId": "61b67bab-69df-459f-ee5b-2b52206ec7a5"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'는': 1, '.': 2, '로': 3, '했습니다': 4, '으로': 5, '경임이': 6, '은경이': 7, '수종이': 8, '필웅이': 9, '이동': 10, '가버렸습니다': 11, '뛰어갔습니다': 12, '복귀': 13, '화장실': 14, '정원': 15, '복도': 16, '갔습니다': 17, '사무실': 18, '부엌': 19, '침실': 20, '어디': 21, '야': 22, '?': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "mCDO93srbtav",
        "outputId": "5e68eee5-3f63-455a-f54a-407e07737c89"
      },
      "source": [
        "idx2word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: '는',\n",
              " 2: '.',\n",
              " 3: '로',\n",
              " 4: '했습니다',\n",
              " 5: '으로',\n",
              " 6: '경임이',\n",
              " 7: '은경이',\n",
              " 8: '수종이',\n",
              " 9: '필웅이',\n",
              " 10: '이동',\n",
              " 11: '가버렸습니다',\n",
              " 12: '뛰어갔습니다',\n",
              " 13: '복귀',\n",
              " 14: '화장실',\n",
              " 15: '정원',\n",
              " 16: '복도',\n",
              " 17: '갔습니다',\n",
              " 18: '사무실',\n",
              " 19: '부엌',\n",
              " 20: '침실',\n",
              " 21: '어디',\n",
              " 22: '야',\n",
              " 23: '?'}"
            ]
          },
          "execution_count": 82,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sTflyTBRb0UH",
        "outputId": "8bec16b5-d79d-4b3a-c75b-6fb057155775"
      },
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oWnDG29FcFzW",
        "outputId": "4a3a7cd1-320e-4995-dfdc-eea0c359c98b"
      },
      "source": [
        "print('스토리의 최대 길이 :',story_max_len)\n",
        "print('질문의 최대 길이 :',question_max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스토리의 최대 길이 : 70\n",
            "질문의 최대 길이 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwwzIsKmHH66"
      },
      "source": [
        "## 정수 인코딩 및 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVtQY63LcG6W"
      },
      "source": [
        "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
        "    Xs, Xq, Y = [], [], []\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    stories, questions, answers = data\n",
        "    for story, question, answer in zip(stories, questions, answers):\n",
        "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
        "        xq = [word2idx[w] for w in tokenize(question)]\n",
        "        Xs.append(xs)\n",
        "        Xq.append(xq)\n",
        "        Y.append(word2idx[answer])\n",
        "\n",
        "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
        "        # 정답은 원-핫 인코딩\n",
        "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
        "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
        "           to_categorical(Y, num_classes=len(word2idx) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8toMbSHcISP"
      },
      "source": [
        "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
        "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "puLdisufcJVu",
        "outputId": "7b13564d-16b0-446e-e91f-d1836917b214"
      },
      "source": [
        "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 70) (10000, 5) (10000, 24) (1000, 70) (1000, 5) (1000, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvYHNf2tHJwj"
      },
      "source": [
        "## 메모리 네트워크 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey3XSg9AcKhu"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWqGB8WJcLwu"
      },
      "source": [
        "# 에포크 횟수\n",
        "train_epochs = 120\n",
        "# 배치 크기\n",
        "batch_size = 32\n",
        "# 임베딩 크기\n",
        "embed_size = 50\n",
        "# LSTM의 크기\n",
        "lstm_size = 64\n",
        "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
        "dropout_rate = 0.30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KiSgI55icNJu",
        "outputId": "d0ce9b17-ef22-45c9-f37f-522f5180baeb"
      },
      "source": [
        "# 플레이스 홀더. 입력을 담는 변수\n",
        "input_sequence = Input((story_max_len,))\n",
        "question = Input((question_max_len,))\n",
        "\n",
        "print('Stories :', input_sequence)\n",
        "print('Question:', question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stories : Tensor(\"input_1:0\", shape=(None, 70), dtype=float32)\n",
            "Question: Tensor(\"input_2:0\", shape=(None, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUeW2Jt1cPSV"
      },
      "source": [
        "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embed_size))\n",
        "input_encoder_m.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
        "\n",
        "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
        "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=question_max_len))\n",
        "input_encoder_c.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvbx-92tcQt-"
      },
      "source": [
        "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=embed_size,\n",
        "                               input_length=question_max_len))\n",
        "question_encoder.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "E5dctAXZcR6d",
        "outputId": "24e0747b-a9a8-4699-8563-7ef1222b6a96"
      },
      "source": [
        "# 실질적인 임베딩 과정\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)\n",
        "\n",
        "print('Input encoded m', input_encoded_m)\n",
        "print('Input encoded c', input_encoded_c)\n",
        "print('Question encoded', question_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input encoded m Tensor(\"sequential/Identity:0\", shape=(None, 70, 50), dtype=float32)\n",
            "Input encoded c Tensor(\"sequential_1/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
            "Question encoded Tensor(\"sequential_2/Identity:0\", shape=(None, 5, 50), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e-L-up6FcVj-",
        "outputId": "02dae2aa-b2ea-44b9-c554-d85ff861498c"
      },
      "source": [
        "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
        "# 유사도는 내적을 사용한다.\n",
        "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
        "match = Activation('softmax')(match)\n",
        "print('Match shape', match)\n",
        "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
        "\n",
        "# add the match matrix with the second input vector sequence\n",
        "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
        "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
        "print('Response shape', response)\n",
        "\n",
        "# concatenate the response vector with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])\n",
        "print('Answer shape', answer)\n",
        "\n",
        "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
        "answer = Dropout(dropout_rate)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
        "# we output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match shape Tensor(\"activation/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
            "Response shape Tensor(\"permute/Identity:0\", shape=(None, 5, 70), dtype=float32)\n",
            "Answer shape Tensor(\"concatenate/Identity:0\", shape=(None, 5, 120), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EW5px9B4cYUV",
        "outputId": "9ee7cf2d-2c52-4f85-82b8-356dd28e9a10"
      },
      "source": [
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# start training the model\n",
        "history = model.fit([Xstrain, Xqtrain],\n",
        "         Ytrain, batch_size, train_epochs,\n",
        "         validation_data=([Xstest, Xqtest], Ytest))\n",
        "\n",
        "# save model\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         multiple             1200        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 5, 50)        1200        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 70, 5)        0           sequential[1][0]                 \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 70, 5)        0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             120         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 70, 5)        0           activation[0][0]                 \n",
            "                                                                 sequential_1[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 5, 70)        0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 120)       0           permute[0][0]                    \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 64)           47360       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 24)           1560        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 24)           0           dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 51,440\n",
            "Trainable params: 51,440\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/120\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 1.8982 - acc: 0.1693 - val_loss: 1.7868 - val_acc: 0.2470\n",
            "Epoch 2/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.7075 - acc: 0.2567 - val_loss: 1.6049 - val_acc: 0.3870\n",
            "Epoch 3/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5579 - acc: 0.3737 - val_loss: 1.4619 - val_acc: 0.4140\n",
            "Epoch 4/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.4848 - acc: 0.4171 - val_loss: 1.4182 - val_acc: 0.4510\n",
            "Epoch 5/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4464 - acc: 0.4446 - val_loss: 1.4419 - val_acc: 0.4200\n",
            "Epoch 6/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.4159 - acc: 0.4557 - val_loss: 1.3817 - val_acc: 0.4640\n",
            "Epoch 7/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.3700 - acc: 0.4781 - val_loss: 1.3279 - val_acc: 0.5040\n",
            "Epoch 8/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3283 - acc: 0.4939 - val_loss: 1.2875 - val_acc: 0.5190\n",
            "Epoch 9/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.3025 - acc: 0.5037 - val_loss: 1.2792 - val_acc: 0.5020\n",
            "Epoch 10/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2779 - acc: 0.5098 - val_loss: 1.2808 - val_acc: 0.4930\n",
            "Epoch 11/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2613 - acc: 0.5132 - val_loss: 1.2507 - val_acc: 0.5360\n",
            "Epoch 12/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2470 - acc: 0.5163 - val_loss: 1.2421 - val_acc: 0.5180\n",
            "Epoch 13/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2273 - acc: 0.5204 - val_loss: 1.2253 - val_acc: 0.5200\n",
            "Epoch 14/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2214 - acc: 0.5276 - val_loss: 1.2376 - val_acc: 0.5270\n",
            "Epoch 15/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2049 - acc: 0.5250 - val_loss: 1.2172 - val_acc: 0.5220\n",
            "Epoch 16/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2003 - acc: 0.5222 - val_loss: 1.2091 - val_acc: 0.5170\n",
            "Epoch 17/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1844 - acc: 0.5322 - val_loss: 1.2151 - val_acc: 0.5180\n",
            "Epoch 18/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1694 - acc: 0.5303 - val_loss: 1.2158 - val_acc: 0.5210\n",
            "Epoch 19/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1671 - acc: 0.5376 - val_loss: 1.2055 - val_acc: 0.5280\n",
            "Epoch 20/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1515 - acc: 0.5399 - val_loss: 1.2079 - val_acc: 0.5250\n",
            "Epoch 21/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1460 - acc: 0.5387 - val_loss: 1.2003 - val_acc: 0.5140\n",
            "Epoch 22/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1382 - acc: 0.5410 - val_loss: 1.2025 - val_acc: 0.5140\n",
            "Epoch 23/120\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.1280 - acc: 0.5433 - val_loss: 1.1976 - val_acc: 0.5100\n",
            "Epoch 24/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1137 - acc: 0.5499 - val_loss: 1.2152 - val_acc: 0.5300\n",
            "Epoch 25/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1129 - acc: 0.5511 - val_loss: 1.2064 - val_acc: 0.5190\n",
            "Epoch 26/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1060 - acc: 0.5502 - val_loss: 1.2110 - val_acc: 0.5210\n",
            "Epoch 27/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0963 - acc: 0.5571 - val_loss: 1.2037 - val_acc: 0.5250\n",
            "Epoch 28/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0846 - acc: 0.5529 - val_loss: 1.2028 - val_acc: 0.5170\n",
            "Epoch 29/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0789 - acc: 0.5555 - val_loss: 1.2014 - val_acc: 0.5100\n",
            "Epoch 30/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0681 - acc: 0.5614 - val_loss: 1.2123 - val_acc: 0.5170\n",
            "Epoch 31/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0644 - acc: 0.5656 - val_loss: 1.2151 - val_acc: 0.5170\n",
            "Epoch 32/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0516 - acc: 0.5719 - val_loss: 1.2093 - val_acc: 0.5210\n",
            "Epoch 33/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0395 - acc: 0.5749 - val_loss: 1.2333 - val_acc: 0.5160\n",
            "Epoch 34/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0355 - acc: 0.5745 - val_loss: 1.2217 - val_acc: 0.5180\n",
            "Epoch 35/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0319 - acc: 0.5769 - val_loss: 1.2105 - val_acc: 0.5160\n",
            "Epoch 36/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0250 - acc: 0.5721 - val_loss: 1.2358 - val_acc: 0.5150\n",
            "Epoch 37/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0151 - acc: 0.5820 - val_loss: 1.2152 - val_acc: 0.5130\n",
            "Epoch 38/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0022 - acc: 0.5903 - val_loss: 1.2352 - val_acc: 0.5190\n",
            "Epoch 39/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9950 - acc: 0.5869 - val_loss: 1.2399 - val_acc: 0.4980\n",
            "Epoch 40/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9837 - acc: 0.5938 - val_loss: 1.2542 - val_acc: 0.5020\n",
            "Epoch 41/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9710 - acc: 0.6014 - val_loss: 1.2608 - val_acc: 0.4990\n",
            "Epoch 42/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9587 - acc: 0.6077 - val_loss: 1.2451 - val_acc: 0.5060\n",
            "Epoch 43/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9472 - acc: 0.6113 - val_loss: 1.2598 - val_acc: 0.5020\n",
            "Epoch 44/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9454 - acc: 0.6126 - val_loss: 1.2911 - val_acc: 0.4950\n",
            "Epoch 45/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9342 - acc: 0.6181 - val_loss: 1.2848 - val_acc: 0.4920\n",
            "Epoch 46/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9166 - acc: 0.6267 - val_loss: 1.2639 - val_acc: 0.5150\n",
            "Epoch 47/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.8987 - acc: 0.6323 - val_loss: 1.2585 - val_acc: 0.5180\n",
            "Epoch 48/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.8820 - acc: 0.6481 - val_loss: 1.2005 - val_acc: 0.5480\n",
            "Epoch 49/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.8306 - acc: 0.6789 - val_loss: 1.1183 - val_acc: 0.6080\n",
            "Epoch 50/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.7546 - acc: 0.7191 - val_loss: 0.9644 - val_acc: 0.6590\n",
            "Epoch 51/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.6478 - acc: 0.7680 - val_loss: 0.8642 - val_acc: 0.6960\n",
            "Epoch 52/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.5953 - acc: 0.7882 - val_loss: 0.7975 - val_acc: 0.7250\n",
            "Epoch 53/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5391 - acc: 0.8089 - val_loss: 0.7459 - val_acc: 0.7310\n",
            "Epoch 54/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5151 - acc: 0.8147 - val_loss: 0.7087 - val_acc: 0.7480\n",
            "Epoch 55/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4799 - acc: 0.8273 - val_loss: 0.6861 - val_acc: 0.7570\n",
            "Epoch 56/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.4362 - acc: 0.8423 - val_loss: 0.6764 - val_acc: 0.7580\n",
            "Epoch 57/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4055 - acc: 0.8540 - val_loss: 0.6075 - val_acc: 0.7880\n",
            "Epoch 58/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3672 - acc: 0.8681 - val_loss: 0.5702 - val_acc: 0.7950\n",
            "Epoch 59/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3476 - acc: 0.8743 - val_loss: 0.5195 - val_acc: 0.8060\n",
            "Epoch 60/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3306 - acc: 0.8822 - val_loss: 0.5008 - val_acc: 0.8180\n",
            "Epoch 61/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.4881 - val_acc: 0.8210\n",
            "Epoch 62/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2943 - acc: 0.8948 - val_loss: 0.4893 - val_acc: 0.8200\n",
            "Epoch 63/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2711 - acc: 0.9023 - val_loss: 0.4696 - val_acc: 0.8310\n",
            "Epoch 64/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2590 - acc: 0.9074 - val_loss: 0.4844 - val_acc: 0.8270\n",
            "Epoch 65/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2523 - acc: 0.9122 - val_loss: 0.4768 - val_acc: 0.8360\n",
            "Epoch 66/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2380 - acc: 0.9167 - val_loss: 0.4523 - val_acc: 0.8420\n",
            "Epoch 67/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2282 - acc: 0.9203 - val_loss: 0.4842 - val_acc: 0.8310\n",
            "Epoch 68/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2187 - acc: 0.9202 - val_loss: 0.4729 - val_acc: 0.8320\n",
            "Epoch 69/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2094 - acc: 0.9247 - val_loss: 0.4587 - val_acc: 0.8450\n",
            "Epoch 70/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2014 - acc: 0.9281 - val_loss: 0.4453 - val_acc: 0.8560\n",
            "Epoch 71/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1968 - acc: 0.9297 - val_loss: 0.4455 - val_acc: 0.8560\n",
            "Epoch 72/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1782 - acc: 0.9345 - val_loss: 0.4398 - val_acc: 0.8550\n",
            "Epoch 73/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1735 - acc: 0.9378 - val_loss: 0.4628 - val_acc: 0.8540\n",
            "Epoch 74/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1653 - acc: 0.9423 - val_loss: 0.4432 - val_acc: 0.8570\n",
            "Epoch 75/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1595 - acc: 0.9426 - val_loss: 0.4374 - val_acc: 0.8590\n",
            "Epoch 76/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1544 - acc: 0.9452 - val_loss: 0.4240 - val_acc: 0.8610\n",
            "Epoch 77/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1514 - acc: 0.9462 - val_loss: 0.4426 - val_acc: 0.8630\n",
            "Epoch 78/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1356 - acc: 0.9524 - val_loss: 0.4601 - val_acc: 0.8640\n",
            "Epoch 79/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1364 - acc: 0.9504 - val_loss: 0.4300 - val_acc: 0.8650\n",
            "Epoch 80/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1339 - acc: 0.9534 - val_loss: 0.4160 - val_acc: 0.8800\n",
            "Epoch 81/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1264 - acc: 0.9555 - val_loss: 0.4788 - val_acc: 0.8650\n",
            "Epoch 82/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1164 - acc: 0.9595 - val_loss: 0.4635 - val_acc: 0.8620\n",
            "Epoch 83/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1194 - acc: 0.9589 - val_loss: 0.4393 - val_acc: 0.8820\n",
            "Epoch 84/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1023 - acc: 0.9654 - val_loss: 0.4558 - val_acc: 0.8750\n",
            "Epoch 85/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1129 - acc: 0.9615 - val_loss: 0.4342 - val_acc: 0.8820\n",
            "Epoch 86/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1029 - acc: 0.9653 - val_loss: 0.4146 - val_acc: 0.8840\n",
            "Epoch 87/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0970 - acc: 0.9671 - val_loss: 0.4296 - val_acc: 0.8870\n",
            "Epoch 88/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0982 - acc: 0.9670 - val_loss: 0.4720 - val_acc: 0.8810\n",
            "Epoch 89/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0862 - acc: 0.9709 - val_loss: 0.4282 - val_acc: 0.8880\n",
            "Epoch 90/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0855 - acc: 0.9706 - val_loss: 0.4450 - val_acc: 0.8880\n",
            "Epoch 91/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0950 - acc: 0.9683 - val_loss: 0.4410 - val_acc: 0.8780\n",
            "Epoch 92/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0879 - acc: 0.9688 - val_loss: 0.4145 - val_acc: 0.8930\n",
            "Epoch 93/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0865 - acc: 0.9716 - val_loss: 0.4516 - val_acc: 0.8850\n",
            "Epoch 94/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0792 - acc: 0.9748 - val_loss: 0.4393 - val_acc: 0.8800\n",
            "Epoch 95/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0753 - acc: 0.9749 - val_loss: 0.4709 - val_acc: 0.8910\n",
            "Epoch 96/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0765 - acc: 0.9750 - val_loss: 0.4645 - val_acc: 0.8870\n",
            "Epoch 97/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0679 - acc: 0.9785 - val_loss: 0.4555 - val_acc: 0.8870\n",
            "Epoch 98/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0674 - acc: 0.9767 - val_loss: 0.4529 - val_acc: 0.8950\n",
            "Epoch 99/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0663 - acc: 0.9773 - val_loss: 0.4423 - val_acc: 0.8960\n",
            "Epoch 100/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0636 - acc: 0.9780 - val_loss: 0.4493 - val_acc: 0.8920\n",
            "Epoch 101/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0660 - acc: 0.9770 - val_loss: 0.4228 - val_acc: 0.9000\n",
            "Epoch 102/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0649 - acc: 0.9793 - val_loss: 0.4401 - val_acc: 0.8960\n",
            "Epoch 103/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0566 - acc: 0.9801 - val_loss: 0.4814 - val_acc: 0.8980\n",
            "Epoch 104/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0544 - acc: 0.9828 - val_loss: 0.4836 - val_acc: 0.8960\n",
            "Epoch 105/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0556 - acc: 0.9819 - val_loss: 0.5037 - val_acc: 0.8950\n",
            "Epoch 106/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0553 - acc: 0.9816 - val_loss: 0.4691 - val_acc: 0.8950\n",
            "Epoch 107/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0516 - acc: 0.9830 - val_loss: 0.4764 - val_acc: 0.8950\n",
            "Epoch 108/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0500 - acc: 0.9834 - val_loss: 0.5337 - val_acc: 0.8800\n",
            "Epoch 109/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0528 - acc: 0.9834 - val_loss: 0.4939 - val_acc: 0.8960\n",
            "Epoch 110/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0483 - acc: 0.9845 - val_loss: 0.5018 - val_acc: 0.8910\n",
            "Epoch 111/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0495 - acc: 0.9855 - val_loss: 0.4642 - val_acc: 0.8980\n",
            "Epoch 112/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.5153 - val_acc: 0.8990\n",
            "Epoch 113/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0465 - acc: 0.9870 - val_loss: 0.5266 - val_acc: 0.8990\n",
            "Epoch 114/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0464 - acc: 0.9854 - val_loss: 0.4907 - val_acc: 0.9020\n",
            "Epoch 115/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0417 - acc: 0.9864 - val_loss: 0.5617 - val_acc: 0.8920\n",
            "Epoch 116/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0362 - acc: 0.9881 - val_loss: 0.4869 - val_acc: 0.9030\n",
            "Epoch 117/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0348 - acc: 0.9891 - val_loss: 0.4923 - val_acc: 0.9060\n",
            "Epoch 118/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0401 - acc: 0.9893 - val_loss: 0.5041 - val_acc: 0.9020\n",
            "Epoch 119/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0365 - acc: 0.9871 - val_loss: 0.4627 - val_acc: 0.9050\n",
            "Epoch 120/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0397 - acc: 0.9883 - val_loss: 0.5235 - val_acc: 0.9060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Iq7PYePXccGu",
        "outputId": "01a53dc9-46d7-4ac5-c40e-de5ec1e31b1a"
      },
      "source": [
        "# plot accuracy and loss plot\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# labels\n",
        "ytest = np.argmax(Ytest, axis=1)\n",
        "\n",
        "# get predictions\n",
        "Ytest_ = model.predict([Xstest, Xqtest])\n",
        "ytest_ = np.argmax(Ytest_, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "Fq0_LEp9eIA2",
        "outputId": "9451512d-05c8-40aa-eaac-48e305950309"
      },
      "source": [
        "NUM_DISPLAY = 30\n",
        "\n",
        "print(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
        "print(39 * \"-\")\n",
        "\n",
        "for i in range(NUM_DISPLAY):\n",
        "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
        "    label = idx2word[ytest[i]]\n",
        "    prediction = idx2word[ytest_[i]]\n",
        "    print(\"{:20}: {:7} {}\".format(question, label, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "질문                |실제값  |예측값\n",
            "---------------------------------------\n",
            "은경이 는 어디 야 ?        : 복도      복도\n",
            "필웅이 는 어디 야 ?        : 화장실     화장실\n",
            "경임이 는 어디 야 ?        : 부엌      부엌\n",
            "경임이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 부엌      부엌\n",
            "경임이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 정원      정원\n",
            "수종이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 사무실     복도\n",
            "수종이 는 어디 야 ?        : 사무실     복도\n",
            "필웅이 는 어디 야 ?        : 부엌      부엌\n",
            "필웅이 는 어디 야 ?        : 정원      정원\n",
            "수종이 는 어디 야 ?        : 사무실     사무실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "은경이 는 어디 야 ?        : 부엌      부엌\n",
            "은경이 는 어디 야 ?        : 정원      정원\n",
            "은경이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 사무실     부엌\n",
            "은경이 는 어디 야 ?        : 부엌      정원\n",
            "필웅이 는 어디 야 ?        : 복도      복도\n",
            "은경이 는 어디 야 ?        : 사무실     사무실\n",
            "은경이 는 어디 야 ?        : 사무실     사무실\n",
            "경임이 는 어디 야 ?        : 복도      사무실\n",
            "수종이 는 어디 야 ?        : 침실      침실\n",
            "경임이 는 어디 야 ?        : 침실      침실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "수종이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 부엌      복도\n"
          ]
        }
      ]
    }
  ]
}